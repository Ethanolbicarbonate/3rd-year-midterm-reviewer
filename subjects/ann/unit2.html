<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unit 2: Neural Networks - CCS248 ANN</title>
    <link rel="stylesheet" href="../../css/styles.css">
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav class="main-nav">
        <div class="nav-container">
            <div class="logo">
                <a href="../../index.html">üìö Komsai Reviewer</a>
            </div>
            <div class="nav-menu" id="navMenu">
                <!-- Dynamically populated by JS -->
            </div>
            <div class="hamburger" id="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <div class="chapter-container">
        <aside class="sidebar" id="sidebar">
            <h3>Quick Navigation</h3>
            <div class="section-links" id="sectionLinks">
                <!-- Auto-populated based on content sections -->
            </div>
        </aside>

        <main class="chapter-main">
            <header class="chapter-header">
                <div class="breadcrumb">
                    <a href="../../index.html">Home</a> / 
                    <span>CCS248 ANN</span> / 
                    <span>Unit 2: Neural Networks</span>
                </div>
                <h1 class="chapter-title">Unit 2: Neural Networks</h1>
            </header>

            <section class="content-area">
                <!-- Main content sections -->
                <div class="content-section">
                    <h2 class="section-title">Hidden Layers and Dense Networks</h2>
                    <div class="content-block info-block">
                        <p>A neural network consists of a group of interconnected neurons, where the output of one layer of neurons becomes the input for the next. The individual neurons are often referred to as <strong>nodes</strong>.</p>
                    </div>
                    <div class="concept-card primary-concept">
                        <h3>Dense Layers (Fully Connected Layers)</h3>
                        <p>This is the most common type of layer in a neural network. It's called "dense" because every neuron in the current layer is connected to every neuron from the previous layer. This creates a dense web of connections, allowing the network to learn complex relationships between features.</p>
                    </div>
                    <br>
                    <h3>Handling Parameters with Matrices</h3>
                    <div class="content-block">
                        <p>For a single perceptron, vectors (lists) are sufficient for managing one set of weights and a bias. However, with hidden layers containing multiple neurons, the number of parameters grows rapidly. The most intuitive and efficient way to handle these multiple sets of weights and biases is with <strong>matrices</strong> (lists of lists or 2D arrays).</p>
                        <p>The forward pass through a dense layer can be expressed as a single matrix operation:</p>
                        \[ \mathbf{Z} = \mathbf{W}^T \mathbf{X} + \mathbf{b} \]
                        <p>Where \(\mathbf{Z}\) is the vector of weighted sums for the layer, \(\mathbf{W}^T\) is the transposed matrix of weights, \(\mathbf{X}\) is the input vector, and \(\mathbf{b}\) is the vector of biases.</p>
                    </div>
                </div>

                <div class="content-section">
                    <h2 class="section-title">Activation Functions</h2>
                    <div class="content-block info-block">
                        <p>Activation functions introduce <strong>non-linearity</strong> into the network. Without them, a neural network, no matter how many layers it has, would behave just like a simple linear regression model. This non-linearity allows the network to learn and model complex, real-world relationships in data.</p>
                    </div>
                    <div class="concept-cards">
                        <div class="concept-card secondary-concept">
                            <h4>Step Function</h4>
                            <p><strong>Formula:</strong> \( f(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases} \)</p>
                            <p><strong>Notes:</strong> The original activation for perceptrons. Rarely used now because its derivative is zero everywhere else, which prevents effective learning with backpropagation.</p>
                        </div>
                         <div class="concept-card secondary-concept">
                            <h4>Linear Function</h4>
                            <p><strong>Formula:</strong> \( f(x) = x \)</p>
                            <p><strong>Notes:</strong> Essentially no activation. Stacking layers with linear activation is equivalent to a single linear layer.</p>
                        </div>
                        <div class="concept-card secondary-concept">
                            <h4>Sigmoid (Logistic)</h4>
                            <p><strong>Formula:</strong> \( f(x) = \frac{1}{1 + e^{-x}} \)</p>
                            <p><strong>Notes:</strong> Squashes output to a range of (0, 1). Ideal for binary classification output layers to represent probability. Suffers from the <strong>vanishing gradient problem</strong>.</p>
                        </div>
                        <div class="concept-card secondary-concept">
                            <h4>Tanh (Hyperbolic Tangent)</h4>
                            <p><strong>Formula:</strong> \( f(x) = \tanh(x) \)</p>
                            <p><strong>Notes:</strong> Squashes output to a range of (-1, 1). Its zero-centered output often makes it preferable to Sigmoid for hidden layers, but it also suffers from vanishing gradients.</p>
                        </div>
                        <div class="concept-card primary-concept">
                            <h4>ReLU (Rectified Linear Unit)</h4>
                            <p><strong>Formula:</strong> \( f(x) = \max(0, x) \)</p>
                            <p><strong>Notes:</strong> The most widely used activation function for hidden layers. It's computationally efficient and helps mitigate the vanishing gradient problem. However, it can suffer from the <strong>dying ReLU problem</strong>, where neurons get stuck outputting zero.</p>
                        </div>
                        <div class="concept-card success note-block">
                            <h4>Leaky ReLU</h4>
                            <p><strong>Formula:</strong> \( f(x) = \max(\alpha x, x) \)</p>
                            <p><strong>Notes:</strong> A variation of ReLU that solves the dying ReLU problem by allowing a small, non-zero gradient (e.g., \(\alpha=0.01\)) for negative inputs.</p>
                        </div>
                        <div class="concept-card primary-concept">
                            <h4>Softmax</h4>
                             <p><strong>Formula:</strong> \( S_{i,j} = \frac{e^{z_{i,j}}}{\sum_{l=1}^{L} e^{z_{i,l}}} \)</p>
                            <p><strong>Notes:</strong> Used exclusively for the output layer in <strong>multi-class classification</strong> problems. It converts a vector of raw scores (logits) into a probability distribution, where the sum of all outputs is 1.</p>
                        </div>
                    </div>
                </div>

                <div class="content-section">
                    <h2 class="section-title">Loss Functions</h2>
                    <div class="content-block info-block">
                        <p>For a model to "learn," it needs to know how wrong its predictions are. A <strong>loss function</strong> (or cost function) is an algorithm that quantifies this error by comparing the model's prediction to the true target value. The goal of training is to adjust the network's weights and biases to minimize this loss.</p>
                    </div>
                    <div class="concept-cards">
                        <div class="concept-card secondary-concept">
                            <h4>Mean Squared Error (MSE)</h4>
                            <p><strong>Use Case:</strong> Regression problems.</p>
                            <p>Calculates the average of the squared differences between predicted and true values. It is highly sensitive to outliers.</p>
                        </div>
                        <div class="concept-card secondary-concept">
                            <h4>Mean Absolute Error (MAE)</h4>
                            <p><strong>Use Case:</strong> Regression problems.</p>
                            <p>Calculates the average of the absolute differences. It is less sensitive to outliers than MSE.</p>
                        </div>
                         <div class="concept-card primary-concept">
                            <h4>Categorical Cross-Entropy</h4>
                            <p><strong>Use Case:</strong> Multi-class classification problems.</p>
                            <p>This is the standard loss function for classification tasks where predictions are probabilities for multiple classes. It is typically paired with a <strong>Softmax</strong> activation function in the output layer.</p>
                        </div>
                    </div>
                    <h3>Accuracy Calculation</h3>
                    <p>While loss guides the training process, <strong>accuracy</strong> is a common metric used to evaluate a model's performance in human-readable terms. It describes how often the model's prediction (the class with the highest confidence) matches the correct class, expressed as a fraction or percentage.</p>
                </div>
            </section>

            <section class="memorization-area">
                <h2 class="mem-header">üìù Quick Review & Memorization</h2>
                
                <div class="mem-section">
                    <h3>Key Terms & Acronyms</h3>
                    <ol>
                        <li><span class="term">Dense Layer</span>: A fully connected layer where every neuron is connected to every neuron in the previous layer.</li>
                        <li><span class="term">Activation Function</span>: A function that introduces non-linearity, allowing networks to learn complex patterns.</li>
                        <li><span class="term">ReLU</span>: Rectified Linear Unit. The most common activation for hidden layers. Formula: \( \max(0, x) \).</li>
                        <li><span class="term">Sigmoid</span>: An activation that squashes outputs to (0, 1), used for binary classification probabilities.</li>
                        <li><span class="term">Softmax</span>: An activation for the output layer in multi-class classification that creates a probability distribution.</li>
                        <li><span class="term">Vanishing Gradient Problem</span>: An issue where gradients become extremely small in deep networks, effectively stopping learning. Affects Sigmoid and Tanh.</li>
                        <li><span class="term">Dying ReLU Problem</span>: An issue where ReLU neurons become inactive if their input is always negative, causing them to stop learning.</li>
                        <li><span class="term">Loss Function</span>: An algorithm that measures the error between a model's prediction and the true answer.</li>
                        <li><span class="term">Categorical Cross-Entropy</span>: The standard loss function for multi-class classification problems.</li>
                        <li><span class="term">Accuracy</span>: A metric that measures the fraction of correct predictions.</li>
                    </ol>
                </div>

                <div class="mem-section">
                    <h3>Summary Points</h3>
                    <ol>
                        <li>Neural networks are built from layers of neurons, with <strong>Dense (Fully Connected)</strong> layers being the most common type.</li>
                        <li>Matrices are used to efficiently handle the large number of weights and biases in a network and to compute a layer's output with matrix multiplication.</li>
                        <li><strong>Activation Functions</strong> are essential for introducing non-linearity, which is what gives neural networks their power to model complex data.</li>
                        <li><strong>ReLU</strong> is the default choice for hidden layers due to its efficiency and ability to combat vanishing gradients, though it has its own "dying ReLU" problem.</li>
                        <li>The <strong>Softmax</strong> activation function is specifically used for the output layer of multi-class classification networks to produce probabilities.</li>
                        <li>A <strong>Loss Function</strong> quantifies the model's error, and the goal of training is to minimize this value through optimization algorithms like gradient descent.</li>
                        <li><strong>Categorical Cross-Entropy</strong> is the go-to loss function for multi-class classification, perfectly complementing the Softmax activation.</li>
                    </ol>
                </div>

                <div class="mem-section">
                    <h3>Important Enumerations</h3>
                    <ol>
                        <li><strong>Key Purposes of Activation Functions:</strong>
                            <ul>
                                <li>Introduce Non-linearity</li>
                                <li>Enable Learning of Complex Mappings</li>
                                <li>Control the Output Range</li>
                                <li>Facilitate Backpropagation</li>
                            </ul>
                        </li>
                        <li><strong>Common Activation Functions:</strong>
                             <ul>
                                <li>Step Function</li>
                                <li>Linear</li>
                                <li>Sigmoid</li>
                                <li>Tanh</li>
                                <li>ReLU / Leaky ReLU</li>
                                <li>Softmax</li>
                            </ul>
                        </li>
                         <li><strong>Common Loss Functions by Task:</strong>
                             <ul>
                                <li><strong>Regression:</strong> Mean Squared Error (MSE), Mean Absolute Error (MAE)</li>
                                <li><strong>Binary Classification:</strong> Binary Cross-Entropy</li>
                                <li><strong>Multi-Class Classification:</strong> Categorical Cross-Entropy</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </section>

            <nav class="chapter-nav">
                <a href="#" class="nav-btn prev-btn">‚Üê Previous Chapter</a>
                <a href="#" class="nav-btn next-btn">Next Chapter ‚Üí</a>
            </nav>
        </main>
    </div>

    <script src="../../js/navigation.js"></script>
    <script src="../../js/chapter.js"></script>
</body>
</html>